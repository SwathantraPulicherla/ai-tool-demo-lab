name: AI Test Execution

on:
  issues:
    types: [opened, labeled, edited]
  workflow_dispatch:  # Manual trigger for testing

# Allow GitHub Pages deployment
permissions:
  pages: write
  id-token: write

# Allow one concurrent deployment
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  cleanup-artifacts:
    if: contains(github.event.issue.labels.*.name, 'ai-test-run') || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - name: Delete in-progress github-pages artifacts
        uses: actions/github-script@v7
        with:
          script: |
            const runs = await github.rest.actions.listWorkflowRunsForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              status: 'in_progress',
            });

            for (const run of runs.data.workflow_runs) {
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: run.id,
              });

              for (const artifact of artifacts.data.artifacts) {
                if (artifact.name === 'github-pages') {
                  console.log(`Deleting artifact ${artifact.id} from run ${run.id}`);
                  try {
                    await github.rest.actions.deleteArtifact({
                      owner: context.repo.owner,
                      repo: context.repo.repo,
                      artifact_id: artifact.id,
                    });
                  } catch (error) {
                    console.log(`Failed to delete artifact ${artifact.id}: ${error.message}`);
                  }
                }
              }
            }

  run-tests:
    if: contains(github.event.issue.labels.*.name, 'ai-test-run') || github.event_name == 'workflow_dispatch'
    needs: cleanup-artifacts
    runs-on: ubuntu-latest
    outputs:
      coverage_available: ${{ steps.test-summary.outputs.coverage_available }}
    steps:
      - name: Checkout target repository
        uses: actions/checkout@v4
        with:
          path: target-repo

      - name: Checkout AI tools
        uses: actions/checkout@v4
        with:
          repository: SwathantraPulicherla/ai-test-runner
          path: ai-tools
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Checkout AI test generator
        uses: actions/checkout@v4
        with:
          repository: SwathantraPulicherla/ai-c-test-generator
          path: ai-test-generator
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Upgrade pip and setuptools
        run: |
          python -m pip install --upgrade pip setuptools wheel

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake build-essential lcov

      - name: Install AI test runner
        run: |
          cd ai-tools
          pip install -e .
          # Set PYTHONPATH to include ai-test-generator for imports
          echo "PYTHONPATH=$GITHUB_WORKSPACE/ai-test-generator:$PYTHONPATH" >> $GITHUB_ENV

      - name: Run AI tests
        id: test-execution
        continue-on-error: true
        run: |
          cd target-repo
          # Clean build directory to avoid CMake cache conflicts
          rm -rf build/
          ai-test-runner --verbose 2>&1

      - name: Generate test summary
        id: test-summary
        if: always()
        run: |
          cd target-repo

          # Extract test results from the output
          if [ -d "tests/test_reports" ]; then
            # Look for test summary lines
            TEST_SUMMARY=$(find tests/test_reports -name "*.txt" -exec cat {} \; | grep -E "(Overall Status|Individual Tests|PASSED|FAILED)" | head -10)

            if [ -z "$TEST_SUMMARY" ]; then
              TEST_SUMMARY=$(find tests/test_reports -name "*.txt" -exec head -20 {} \; | head -10)
            fi

            echo "summary<<EOF" >> $GITHUB_OUTPUT
            echo "$TEST_SUMMARY" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "summary=No test reports found" >> $GITHUB_OUTPUT
          fi

          # Check for coverage report
          if [ -d "tests/coverage_reports" ] && [ -f "tests/coverage_reports/index.html" ]; then
            echo "coverage_available=true" >> $GITHUB_OUTPUT
          else
            echo "coverage_available=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate Test Intelligence Report
        id: intelligence-analysis
        if: always() && steps.test-execution.outcome == 'failure'
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          cd target-repo

          # Create intelligence analysis script
          cat > analyze_failures.py << 'EOF'
          import os
          import json
          import sys
          sys.path.append('../ai-test-generator')

          from ai_c_test_generator.intelligence import TestIntelligenceAnalyzer

          def parse_test_results():
              results = {}
              test_reports_dir = "tests/test_reports"

              if not os.path.exists(test_reports_dir):
                  return results

              for filename in os.listdir(test_reports_dir):
                  if filename.endswith('.txt'):
                      test_name = filename.replace('.txt', '')
                      filepath = os.path.join(test_reports_dir, filename)

                      with open(filepath, 'r') as f:
                          content = f.read()

                      # Determine if test passed
                      passed = "PASSED" in content and "FAILED" not in content

                      results[test_name] = {
                          'passed': passed,
                          'error_output': content if not passed else '',
                          'compilation_errors': []  # Could be enhanced to parse actual errors
                      }

              return results

          def main():
              # Get test results
              test_results = parse_test_results()

              # Get source files
              source_files = []
              if os.path.exists('src'):
                  source_files = [f for f in os.listdir('src') if f.endswith('.c') or f.endswith('.h')]

              # Initialize intelligence analyzer
              analyzer = TestIntelligenceAnalyzer(os.environ.get('GEMINI_API_KEY'))

              # Generate intelligence report
              intelligence_report = analyzer.analyze_test_failures(test_results, source_files)

              # Save reports
              os.makedirs('reports', exist_ok=True)
              analyzer.generate_intelligence_report('reports/test_analysis.md', intelligence_report)
              analyzer.generate_fix_priority_csv('reports/fix_priority.csv', intelligence_report)

              # Save JSON report for workflow
              with open('reports/intelligence_report.json', 'w') as f:
                  json.dump(intelligence_report, f, indent=2)

              print("Intelligence analysis complete!")
              print(f"Quality Score: {intelligence_report['quality_metrics']['quality_score']}/100")
              print(f"Time Savings: {intelligence_report['executive_summary']['total_time_savings']}")

          if __name__ == "__main__":
              main()
          EOF

          # Run intelligence analysis
          python analyze_failures.py

      - name: Comment test results on issue
        if: github.event_name == 'issues'
        uses: actions/github-script@v7
        with:
          script: |
            const testExitCode = '${{ steps.test-execution.outcome }}' === 'success';
            const testSummary = '${{ steps.test-summary.outputs.summary }}';
            const coverageAvailable = '${{ steps.test-summary.outputs.coverage_available }}';

            let body;
            if (testExitCode) {
              body = `[SUCCESS] **AI Test Execution Completed Successfully!**\n\n## Test Results\n\`\`\`\n${testSummary}\n\`\`\`\n\nAll tests passed! [SUCCESS]`;
            } else {
              body = `[FAILED] **AI Test Execution Completed with Failures**\n\n## Test Results\n\`\`\`\n${testSummary}\n\`\`\`\n\nSome tests failed. Please check the detailed test reports in the workflow artifacts.`;

              // Add intelligence insights if available
              try {
                const fs = require('fs');
                const intelligenceData = fs.existsSync('target-repo/reports/intelligence_report.json') ?
                  fs.readFileSync('target-repo/reports/intelligence_report.json', 'utf8') : null;

                if (intelligenceData) {
                  const report = JSON.parse(intelligenceData);
                  const qualityScore = report.quality_metrics?.quality_score || 'N/A';
                  const timeSavings = report.executive_summary?.total_time_savings || 'N/A';
                  const topFixes = report.priority_fixes?.slice(0, 3) || [];

                  body += `\n\n## ðŸ” Test Intelligence Analysis\n`;
                  body += `**Quality Score**: ${qualityScore}/100\n`;
                  body += `**Estimated Time Savings**: ${timeSavings}\n\n`;

                  if (topFixes.length > 0) {
                    body += `### ðŸš€ Priority Fixes (Quick Wins)\n`;
                    topFixes.forEach((fix, index) => {
                      body += `${index + 1}. **${fix.test_name}** (${fix.estimated_time})\n`;
                      body += `   - ${fix.root_cause}\n`;
                      body += `   - Impact: ${fix.impact}\n\n`;
                    });
                  }

                  body += `ðŸ“Š **Full Intelligence Report**: Download \`test_analysis.md\` and \`fix_priority.csv\` from workflow artifacts.\n`;
                }
              } catch (error) {
                console.log('Intelligence report not available:', error.message);
              }
            }

            if (coverageAvailable === 'true') {
              body += `\n\n## Coverage Report\n[COVERAGE] Coverage reports are available:\n- **GitHub Pages**: https://${context.repo.owner}.github.io/${context.repo.repo}/\n- **Download Artifacts**: Download the "test-reports" and "coverage-reports" artifacts from this workflow run to view detailed information.`;
            } else {
              body += `\n\n## Test Reports\n[REPORTS] Test reports are available in the "test-reports" artifact from this workflow run.`;
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });

      - name: Upload test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-reports
          path: target-repo/tests/test_reports/
          retention-days: 30

      - name: Upload coverage reports archive
        if: always() && steps.test-summary.outputs.coverage_available == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: target-repo/tests/coverage_reports/
          retention-days: 30

      - name: Upload intelligence reports
        if: always() && steps.intelligence-analysis.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: intelligence-reports
          path: target-repo/reports/
          retention-days: 30

  upload-pages-artifact:
    needs: run-tests
    if: needs.run-tests.outputs.coverage_available == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Remove existing github-pages artifacts for this run
        uses: actions/github-script@v7
        with:
          script: |
            const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
              owner: context.repo.owner,
              repo: context.repo.repo,
              run_id: context.runId,
            });

            for (const artifact of artifacts.data.artifacts) {
              if (artifact.name === 'github-pages') {
                console.log(`Deleting existing artifact ${artifact.id} before upload`);
                await github.rest.actions.deleteArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: artifact.id,
                });
              }
            }

      - name: Download coverage reports artifact
        uses: actions/download-artifact@v4
        with:
          name: coverage-reports
          path: coverage_reports

      - name: Upload coverage reports for Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: coverage_reports/

  deploy-coverage:
    needs: upload-pages-artifact
    if: always() && needs.upload-pages-artifact.result == 'success'
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
